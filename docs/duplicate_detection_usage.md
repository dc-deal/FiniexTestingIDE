# Duplicate Detection & Data Mode System

## ğŸ“‹ Overview

Das System schÃ¼tzt die DatenintegritÃ¤t durch:
1. **Artificial Duplicate Detection** - Erkennt manuell kopierte Parquet-Files
2. **Cross-Directory Detection** - Erkennt Duplikate Ã¼ber data_collector Verzeichnisse hinweg (NEW in C#003)
3. **Data Mode Support** - Steuert das Handling von natÃ¼rlichen Duplikaten

---

## ğŸ—‚ï¸ Directory Structure (NEW in C#003)

### Hierarchische Organisation

```
data/processed/
â”œâ”€â”€ .parquet_index.json          # Zentraler Index
â”œâ”€â”€ mt5/                          # data_collector: MetaTrader 5
â”‚   â”œâ”€â”€ EURUSD/                   # Symbol-spezifische Verzeichnisse
â”‚   â”‚   â”œâ”€â”€ EURUSD_20250923_120000.parquet
â”‚   â”‚   â””â”€â”€ EURUSD_20250923_130000.parquet
â”‚   â”œâ”€â”€ GBPUSD/
â”‚   â””â”€â”€ USDJPY/
â””â”€â”€ ib/                           # data_collector: Interactive Brokers (zukÃ¼nftig)
    â””â”€â”€ EURUSD/
        â””â”€â”€ EURUSD_20250923_120000.parquet
```

### data_collector Feld

**In JSON Metadata (TickCollector v1.0.4):**
```json
{
  "metadata": {
    "symbol": "EURUSD",
    "data_collector": "mt5",      // NEU in v1.0.4
    "data_format_version": "1.0.4",
    "broker": "Vantage...",
    ...
  }
}
```

**In Parquet Metadata:**
```python
parquet_metadata = {
    "source_file": "EURUSD_20250923_120000_ticks.json",
    "symbol": "EURUSD",
    "data_collector": "mt5",      // Wird aus JSON Ã¼bernommen
    "broker": "Vantage...",
    ...
}
```

**Fallback:** Wenn `data_collector` fehlt â†’ automatisch "mt5"

---

## ğŸ“„ Import Behavior

### Normaler Import (C#003)
```bash
python tick_importer.py

Processing: EURUSD_20250923_120000_ticks.json
  â†’ Extracting data_collector: 'mt5'
  â†’ Target: data/processed/mt5/EURUSD/
  â†’ Checking for existing duplicates (all collectors)...
  â†’ No duplicates found âœ…
  â†’ Creating: EURUSD_20250923_120000.parquet
  âœ“ mt5/EURUSD/EURUSD_20250923_120000.parquet: 45,231 Ticks
    Compression 10.2:1 (45.3MB â†’ 4.4MB)
```

### Re-Import Detection - Same Collector
```bash
python tick_importer.py

Processing: EURUSD_20250923_120000_ticks.json
  â†’ Extracting data_collector: 'mt5'
  â†’ Target: data/processed/mt5/EURUSD/
  â†’ Checking for existing duplicates (all collectors)...
  âš ï¸  Found existing Parquet: mt5/EURUSD/EURUSD_20250923_120000.parquet
      Existing: data_collector='mt5' | Importing: data_collector='mt5'
  
ERROR: 
================================================================================
âš ï¸  ARTIFICIAL DUPLICATE DETECTED - DATA INTEGRITY VIOLATION
================================================================================

ğŸ“„ Original Source JSON:
   EURUSD_20250923_120000_ticks.json

ğŸ“¦ Duplicate Parquet Files Found: 1

   [1] mt5/EURUSD/EURUSD_20250923_120000.parquet
       Ticks:          45,231
       Range:     2025-09-23 12:00:00 â†’ 2025-09-23 14:30:45
       Size:            4.42 MB

ğŸ“‹ Parquet Metadata Comparison:

   â€¢ source_file        âœ… IDENTICAL
   â€¢ symbol             âœ… IDENTICAL
   â€¢ data_collector     âœ… IDENTICAL
       [1] mt5
   â€¢ broker             âœ… IDENTICAL
   â€¢ collector_version  âœ… IDENTICAL
   â€¢ tick_count         âœ… IDENTICAL
   â€¢ processed_at       âš ï¸  DIFFERENT
       [1] 2025-09-23T12:05:30
       [2] 2025-09-23T14:32:15

ğŸ”¬ Data Similarity Analysis:
   â€¢ Tick Counts:  âœ… IDENTICAL
   â€¢ Time Ranges:  âœ… IDENTICAL

âš ï¸  ğŸ”´ CRITICAL - Complete data duplication detected
   Impact: Identical files, test results will be severely compromised

ğŸ’¡ Recommended Actions:
   1. DELETE the older file (check processed_at timestamp)
   2. Keep the file with most recent processed_at
   3. Rebuild index: python python/cli/data_index_cli.py rebuild

================================================================================

â†’ Ãœberspringe Import (Duplikat existiert bereits)
```

### Cross-Collector Duplicate Detection (NEW in C#003) ğŸ”¥

```bash
# Scenario: Dieselbe Quelle versehentlich unter anderem Collector importiert

python tick_importer.py

Processing: EURUSD_20250923_120000_ticks.json
  â†’ Extracting data_collector: 'ib'  # Andere Quelle!
  â†’ Target: data/processed/ib/EURUSD/
  â†’ Checking for existing duplicates (all collectors)...
  âš ï¸  Found existing Parquet: mt5/EURUSD/EURUSD_20250923_120000.parquet
      Existing: data_collector='mt5' | Importing: data_collector='ib'
      âš ï¸  CROSS-COLLECTOR DUPLICATE DETECTED!

ERROR:
================================================================================
âš ï¸  ARTIFICIAL DUPLICATE DETECTED - DATA INTEGRITY VIOLATION
================================================================================

ğŸ“„ Original Source JSON:
   EURUSD_20250923_120000_ticks.json

ğŸ“¦ Duplicate Parquet Files Found: 1

   [1] mt5/EURUSD/EURUSD_20250923_120000.parquet
       Ticks:          45,231
       Range:     2025-09-23 12:00:00 â†’ 2025-09-23 14:30:45
       Size:            4.42 MB

ğŸ“‹ Parquet Metadata Comparison:

   â€¢ source_file        âœ… IDENTICAL
   â€¢ symbol             âœ… IDENTICAL
   â€¢ data_collector     âš ï¸  CROSS-COLLECTOR DUPLICATE!
       [1] mt5
       [2] ib
   â€¢ broker             âœ… IDENTICAL
   â€¢ collector_version  âœ… IDENTICAL

âš ï¸  ğŸ”´ CRITICAL - Cross-Collector Duplication
   Impact: Same data imported under different collectors: mt5, ib

ğŸ’¡ Recommended Actions:
   1. INVESTIGATE why the same source was imported under different collectors
   2. DELETE one of the duplicate files (choose the wrong collector)
   3. Check your import workflow to prevent cross-collector duplicates
   4. Rebuild index: python python/cli/data_index_cli.py rebuild

================================================================================
```

---

## ğŸš¨ Artificial Duplicate Detection (Cross-Directory Protection)

### Enhanced Two-Layer Protection (C#003)

#### ğŸ›¡ï¸ Layer 1: Import Prevention (tick_importer.py)
**Sucht Ã¼ber ALLE data_collector Verzeichnisse hinweg**

```python
# VORHER (Old):
existing_files = list(self.target_dir.glob(f"{symbol}_*.parquet"))
# Sucht nur: data/processed/EURUSD_*.parquet

# NACHHER (C#003):
search_pattern = f"*/{symbol}/{symbol}_*.parquet"
existing_files = list(self.target_dir.glob(search_pattern))
# Sucht: data/processed/*/EURUSD/EURUSD_*.parquet
#   â†’ mt5/EURUSD/*.parquet
#   â†’ ib/EURUSD/*.parquet
#   â†’ (alle collector)
```

**Was es erkennt:**
```
Scenario 1: Same Collector Re-Import
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mt5/EURUSD/EURUSD_20250923_120000.parquet    (exists)
mt5/EURUSD/EURUSD_20250923_120000.parquet    (trying to import)
â†’ ğŸ”´ DUPLICATE DETECTED (same collector)

Scenario 2: Cross-Collector Duplicate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mt5/EURUSD/EURUSD_20250923_120000.parquet    (exists)
ib/EURUSD/EURUSD_20250923_120000.parquet     (trying to import)
â†’ ğŸ”´ CROSS-COLLECTOR DUPLICATE!
```

#### ğŸ›¡ï¸ Layer 2: Load Validation (TickDataLoader)
**Validiert beim Laden Ã¼ber alle Collector hinweg**

```python
# Uses index which scans recursively: glob("**/*.parquet")
# LÃ¤dt ALLE Parquet-Files fÃ¼r das Symbol, egal unter welchem collector

files = self.index_manager.get_relevant_files(symbol, start, end)
# Returns: [
#   Path("data/processed/mt5/EURUSD/file1.parquet"),
#   Path("data/processed/ib/EURUSD/file2.parquet"),  # if exists
# ]

# Check for duplicates across ALL files
duplicate_report = self._check_artificial_duplicates(files)
```

### Metadata-Vergleich (Enhanced in C#003)

Der Duplicate Report vergleicht jetzt auch `data_collector`:

```
ğŸ“‹ Parquet Metadata Comparison:

   â€¢ source_file        âœ… IDENTICAL
   â€¢ symbol             âœ… IDENTICAL
   â€¢ data_collector     âš ï¸  CROSS-COLLECTOR DUPLICATE!  // NEU!
       [1] mt5
       [2] ib
   â€¢ broker             âœ… IDENTICAL
```

**Wichtig:** `data_collector` wird **NICHT** als Duplicate-Kriterium verwendet!
- Duplikat = Gleiche `source_file`
- `data_collector` wird nur zur **Info** angezeigt

---

## ğŸ¯ Data Modes

### `data_mode="raw"`
- **Zweck:** Maximaler Realismus fÃ¼r Stress-Tests
- **Verhalten:** Alle Duplikate bleiben erhalten (wie vom Broker empfangen)
- **Use-Case:** Phase 3 Testing, Algo-Stress unter realen Bedingungen

### `data_mode="realistic"`
- **Zweck:** Normale Test-Bedingungen
- **Verhalten:** NatÃ¼rliche Duplikate werden entfernt
- **Use-Case:** Standard-Testing, Performance-Validierung

### `data_mode="clean"`
- **Zweck:** Optimierte Test-Bedingungen  
- **Verhalten:** NatÃ¼rliche Duplikate werden entfernt (wie realistic)
- **Use-Case:** Benchmark-Tests, Clean-Data-Szenarien

---

## ğŸ’» Usage in Code

### In Scenario Config (JSON)

```json
{
  "global": {
    "data_mode": "realistic"
  },
  "scenarios": [
    {
      "name": "EURUSD_stress_test",
      "symbol": "EURUSD",
      "data_mode": "raw"  // Override fÃ¼r dieses Scenario
    }
  ]
}
```

### Direct API Usage

```python
from python.data_worker.data_loader.core import TickDataLoader
from python.data_worker.data_loader.exceptions import ArtificialDuplicateException

loader = TickDataLoader('./data/processed/')

try:
    df = loader.load_symbol_data(
        symbol="EURUSD",
        data_mode="realistic",
        detect_artificial_duplicates=True  # Default
    )
except ArtificialDuplicateException as e:
    print(e.report.get_detailed_report())
    # Cross-Collector duplicate detected!
```
