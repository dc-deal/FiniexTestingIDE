"""
FiniexTestingIDE - Benchmark Certificate Validation
CI/CD test that validates benchmark reports without running benchmarks

This test is designed for CI pipelines:
- Does NOT execute the actual benchmark (too resource-intensive)
- Checks if a valid benchmark report exists
- Validates report is not expired
- Validates report shows PASSED status

Usage in CI:
    pytest tests/mvp_benchmark/test_benchmark_certificate.py

Usage locally (run full benchmark):
    pytest tests/mvp_benchmark/test_throughput_regression.py
"""

import json
import pytest
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List


BENCHMARK_REPORTS_DIR = Path(__file__).parent / "reports"


def _find_latest_report() -> Optional[Path]:
    """
    Find the most recent benchmark report by timestamp.

    Returns:
        Path to latest report or None if no reports exist
    """
    if not BENCHMARK_REPORTS_DIR.exists():
        return None

    reports = list(BENCHMARK_REPORTS_DIR.glob("benchmark_report_*.json"))

    if not reports:
        return None

    # Parse reports and find latest by timestamp field
    latest_report = None
    latest_timestamp = None

    for report_path in reports:
        try:
            with open(report_path, 'r') as f:
                report = json.load(f)

            timestamp_str = report.get("timestamp")
            if timestamp_str:
                timestamp = datetime.fromisoformat(timestamp_str)
                if latest_timestamp is None or timestamp > latest_timestamp:
                    latest_timestamp = timestamp
                    latest_report = report_path
        except (json.JSONDecodeError, KeyError, ValueError):
            continue

    return latest_report


def _load_report(report_path: Path) -> Dict[str, Any]:
    """Load and parse a benchmark report."""
    with open(report_path, 'r') as f:
        return json.load(f)


class TestBenchmarkCertificate:
    """
    CI-friendly tests that validate benchmark certificates.

    These tests do NOT run the actual benchmark - they only check
    if a valid, non-expired benchmark report exists.
    """

    def test_report_exists(self):
        """
        Check if a benchmark report exists in the reports directory.

        DESIGN DECISION: SKIP instead of FAIL when no report exists.

        Rationale:
        - When running `pytest tests/mvp_benchmark/` locally, this test runs
        BEFORE test_throughput_regression.py (alphabetical order)
        - The report is generated by test_throughput_regression.py
        - This creates a chicken-egg problem: certificate tests fail before
        the report-generating tests even run

        Trade-off:
        - SKIP allows local full-suite runs without false failures
        - BUT: CI won't fail if someone forgets to commit the report

        Mitigation:
        - CI should run ONLY test_benchmark_certificate.py (not full suite)
        - If report missing in CI → all 4 tests SKIP → visible in CI output
        - Code review should check for committed report

        Alternative workflows:
        - Local benchmark: pytest tests/mvp_benchmark/test_throughput_regression.py -v
        - CI validation:   pytest tests/mvp_benchmark/test_benchmark_certificate.py -v
        """
        latest_report = _find_latest_report()

        if latest_report is None:
            pytest.skip(
                f"No benchmark report found in {BENCHMARK_REPORTS_DIR}\n"
                f"\n"
                f"To generate a benchmark report:\n"
                f"1. Run: pytest tests/mvp_benchmark/test_throughput_regression.py -v\n"
                f"2. Commit the generated report file\n"
                f"3. Re-run this test"
            )

        print(f"\n✅ Found report: {latest_report.name}")

    def test_report_not_expired(self):
        """
        The latest benchmark report must not be expired.

        Reports expire after 90 days (configurable in benchmark_config.json).
        If expired, re-run the benchmark locally and commit new report.
        """
        latest_report = _find_latest_report()

        if latest_report is None:
            pytest.skip("No report found - see test_report_exists")

        report = _load_report(latest_report)

        valid_until_str = report.get("valid_until")
        assert valid_until_str, "Report missing 'valid_until' field"

        valid_until = datetime.fromisoformat(valid_until_str)
        now = datetime.now(timezone.utc)

        # Make valid_until timezone-aware if it isn't
        if valid_until.tzinfo is None:
            valid_until = valid_until.replace(tzinfo=timezone.utc)

        days_remaining = (valid_until - now).days

        assert now <= valid_until, (
            f"Benchmark report has EXPIRED!\n"
            f"\n"
            f"Report: {latest_report.name}\n"
            f"Created: {report.get('timestamp', 'unknown')}\n"
            f"Expired: {valid_until_str}\n"
            f"Expired {abs(days_remaining)} days ago\n"
            f"\n"
            f"To fix:\n"
            f"1. Run benchmark locally: pytest tests/mvp_benchmark/test_throughput_regression.py\n"
            f"2. Commit the new report file\n"
            f"3. Push to trigger CI again"
        )

        print(f"\n✅ Report valid for {days_remaining} more days")
        print(f"   Expires: {valid_until_str}")

    def test_report_passed(self):
        """
        The latest benchmark report must show PASSED status.

        If status is FAILED, there was a performance regression
        or the benchmark was run in debug mode.
        Investigate and fix before merging.
        """
        latest_report = _find_latest_report()

        if latest_report is None:
            pytest.skip("No report found - see test_report_exists")

        report = _load_report(latest_report)

        overall_status = report.get("overall_status")
        debug_mode = report.get("debug_mode_detected", False)

        # Special handling for debug mode
        if debug_mode:
            pytest.fail(
                f"Benchmark report was generated in DEBUG MODE!\n"
                f"\n"
                f"Report: {latest_report.name}\n"
                f"\n"
                f"Debug mode invalidates all performance measurements.\n"
                f"Re-run the benchmark WITHOUT a debugger attached:\n"
                f"  pytest tests/mvp_benchmark/test_throughput_regression.py -v\n"
            )

        assert overall_status == "PASSED", (
            f"Benchmark report shows FAILED status!\n"
            f"\n"
            f"Report: {latest_report.name}\n"
            f"Status: {overall_status}\n"
            f"\n"
            f"Failed metrics:\n" +
            _format_failed_metrics(report) +
            f"\n"
            f"This indicates a performance regression.\n"
            f"Investigate and fix before merging."
        )

        # Check for warnings
        warnings = report.get("warnings", [])
        if warnings:
            print(f"\n✅ PASSED with {len(warnings)} warning(s):")
            for w in warnings:
                print(f"   ⚠️  {w}")
        else:
            print(f"\n✅ PASSED - no warnings")

        # Print summary
        print(f"\n   System: {report.get('system_id', 'unknown')}")
        print(f"   Commit: {report.get('git_commit', 'unknown')}")

    def test_report_integrity(self):
        """
        Benchmark report should have all required fields.

        Validates report structure is complete.
        """
        latest_report = _find_latest_report()

        if latest_report is None:
            pytest.skip("No report found - see test_report_exists")

        report = _load_report(latest_report)

        required_fields = [
            "timestamp",
            "valid_until",
            "system_id",
            "scenario",
            "debug_mode_detected",
            "overall_status",
            "metrics"
        ]

        missing = [f for f in required_fields if f not in report]

        assert not missing, (
            f"Report missing required fields: {missing}\n"
            f"Report may be corrupted or from an older version.\n"
            f"Re-run the benchmark to generate a new report."
        )

        # Validate metrics structure
        metrics = report.get("metrics", [])
        assert len(metrics) > 0, "Report has no metrics"

        for metric in metrics:
            if metric.get("status") != "INFO":
                assert "tolerance_percent" in metric, (
                    f"Metric {metric.get('name')} missing tolerance_percent"
                )

        print(f"\n✅ Report integrity verified")
        print(f"   {len(metrics)} metrics recorded")


def _format_failed_metrics(report: Dict[str, Any]) -> str:
    """Format failed metrics for error message."""
    metrics = report.get("metrics", [])
    failed = [m for m in metrics if m.get("status") == "FAILED"]

    if not failed:
        return "   (no details available)\n"

    lines = []
    for m in failed:
        lines.append(
            f"   - {m['name']}: {m['measured']} "
            f"(baseline: {m['reference']}, "
            f"deviation: {m['deviation_percent']:+.1f}%, "
            f"tolerance: ±{m['tolerance_percent']}%)"
        )

    return "\n".join(lines) + "\n"
